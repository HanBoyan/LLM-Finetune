{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68f60db-008c-472e-8d78-3c870a375959",
   "metadata": {},
   "source": [
    "## Loading Model & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63a4c8d-f762-4579-b627-64e4d6cd9883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers.models.gemma2.modeling_gemma2 import Gemma2ForCausalLM\n",
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e10c4f4-755f-49e8-b25e-6c3d78b35d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig,PeftModel,get_peft_model,prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import os ,wandb\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10b08d2-e602-40d0-a115-922fcc02f9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"gemma2-2b\"\n",
    "dataset_path = 'databricks-dolly-15k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b368b682-80be-4db6-84c1-019361019604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float32,\n",
    "    bnb_4bit_use_double_quant = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22568d72-cf40-4156-8cf5-5d6f7c97130a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8889ccf4fc2c4eb78b6c0e79cfa83a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06c0fdc6-bf3a-4295-a0b3-5e9a5cd4f7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"context\"]\n",
    "    outputs      = examples[\"response\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + tokenizer.eos_token\n",
    "        #if len(text) <= 10000:\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78155db8-6c0b-4237-94d6-1af92e77e4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(dataset_path,split = 'train')\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b6c0e17-56aa-453d-b85d-4cd2dccc442d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_long_samples(example, max_length):\n",
    "    return len(example['text']) <= max_length\n",
    "\n",
    "\n",
    "filtered_dataset = dataset.filter(lambda example: filter_long_samples(example, max_length=3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f904fa2-8294-4e40-b765-790924a06207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14423"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21a121-c5a0-4f55-9e8c-f10a251266be",
   "metadata": {},
   "source": [
    "## Wandb Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63914180-6fb6-491f-9900-aa423bae846a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25032b44-1355-4d60-864b-3639c598e9d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project = 'Gemma-Lora',\n",
    "    job_type = 'training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e16545a-f0dc-4c94-a708-457f2b65d2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _,param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Params need train:{trainable_params} || Total Params:{all_param} || percentage:{100*(trainable_params/all_param)}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff18dc-0d7a-4e30-b52e-b424ec836e93",
   "metadata": {},
   "source": [
    "## Lora & Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf53c9c-b11b-419f-b385-97d19038971e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lora Config\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\"q_proj\",\"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c05bf4-6fef-412f-b001-611b9bcab906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparamters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = '/root/autodl-tmp/output',\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps = 600,\n",
    "    logging_steps = 200,\n",
    "    save_total_limit = 3,\n",
    "    learning_rate = 2e-4,\n",
    "    weight_decay = 0.001,\n",
    "    fp16 = False,\n",
    "    bf16 = False,\n",
    "    max_grad_norm = 0.3,\n",
    "    max_steps = -1,\n",
    "    warmup_ratio = 0.05,\n",
    "    group_by_length = True,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    report_to = \"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41750c-53e6-4353-9e74-51f48ce6a3a9",
   "metadata": {},
   "source": [
    "## Model Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9859711-bc81-4278-9ab7-def9d81b79c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/root/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = filtered_dataset,\n",
    "    peft_config = peft_config,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    "    packing = False,\n",
    "    max_seq_length=32768,\n",
    "    dataset_text_field=\"text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ce049c-8dd8-460f-a3fa-70dc44e76a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla V100-PCIE-32GB. Max memory = 31.739 GB.\n",
      "4.457 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18eb937a-fa41-4700-9b91-bb4453db63bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3606' max='3606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3606/3606 1:20:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>5.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.588600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3606, training_loss=3.0195249509824627, metrics={'train_runtime': 4860.0294, 'train_samples_per_second': 2.968, 'train_steps_per_second': 0.742, 'total_flos': 3.218520668603904e+16, 'train_loss': 3.0195249509824627, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f68a54e0-d172-4796-9cb6-b7385d56387e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params need train:1597440 || Total Params:1603801344 || percentage:0.09960335835708092%\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model,peft_config)\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef04b2b-bdd3-4f3b-aa78-1512ab10698d",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "490cb83a-8e1f-49eb-bf37-e4f21fb1b3be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>     █  ▂      ▁  </td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>▄▄▆▂▃▅▅█▂▄▁▃▂▃▄▂▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3.218520668603904e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>3606</td></tr><tr><td>train/grad_norm</td><td>nan</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.5886</td></tr><tr><td>train_loss</td><td>3.01952</td></tr><tr><td>train_runtime</td><td>4860.0294</td></tr><tr><td>train_samples_per_second</td><td>2.968</td></tr><tr><td>train_steps_per_second</td><td>0.742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sunset-12</strong> at: <a href='https://wandb.ai/nononono1/Gemma-Lora/runs/l0udpt3z' target=\"_blank\">https://wandb.ai/nononono1/Gemma-Lora/runs/l0udpt3z</a><br/> View project at: <a href='https://wandb.ai/nononono1/Gemma-Lora' target=\"_blank\">https://wandb.ai/nononono1/Gemma-Lora</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240813_185019-l0udpt3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained('lora-model')\n",
    "\n",
    "wandb.finish()\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1eb802-7a2e-490d-9aae-6c729455a169",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82e23492-98d1-4034-a6b1-0b86fcbc2a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stream(user_input):\n",
    "    device = \"cuda:0\"\n",
    "    system_prompt ='Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.'\n",
    "    B_INST,R_INST = \"###Instruction:\\n\",\"###Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{R_INST}\"\n",
    "    inputs = tokenizer([prompt],return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "    _ = model.generate(**inputs,streamer = streamer,max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2bef232-974a-4756-ba57-0c011c520932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wall of China is a series of fortifications built across the northern borders of China to protect the country from invasions. The Great Wall was built over a period of 2,000 years, with the first sections built in the 3rd century BC. The Great Wall is a series of fortifications built across the northern borders of China to protect the country from invasions. The Great Wall was built over a period of 2,000 years, with the first sections built in the 3rd century BC. The Great Wall is a series of fortifications built across the northern borders of China to protect the country from invasions.\n"
     ]
    }
   ],
   "source": [
    "#Base Model\n",
    "stream(\"Give me some info about the Great Wall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d40c8ccd-9aaa-4eb0-89b2-90cc99010fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c15dbe31ef404fa99e0dd1c3c531b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage = True,\n",
    "    return_dict = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17d9f160-4b66-4db4-af29-6a303417ac69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fintuned_model = PeftModel.from_pretrained(model,\"lora-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "704cc318-8260-4879-b33a-68d56d069398",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = fintuned_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e12d94db-6cd3-4477-aa1c-8917f0548a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c356c856-e0f0-4e2f-a3b6-688073320425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_stream(user_input):\n",
    "    device = \"cuda:0\"\n",
    "    system_prompt ='Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.'\n",
    "    B_INST,R_INST = \"###Instruction:\\n\",\"###Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_input.strip()}\\n\\n{R_INST}\"\n",
    "    inputs = tokenizer([prompt],return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "    _ =merged_model.generate(**inputs,streamer = streamer,max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d91dd-67fe-435d-8a44-5705bd117779",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stream(\"Give me some info about the Great Wall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4554054-adf2-4a71-a4a8-857ec3a72bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
